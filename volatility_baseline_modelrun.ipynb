{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYObgrh/S+kYoCGcQ0WTpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atacankavdir/academic/blob/main/volatility_baseline_modelrun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ],
      "metadata": {
        "id": "TVCbzktrdaW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY6dFBnrPIFw",
        "outputId": "5f4ca35b-2a7e-4164-9394-28acc66d20aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXJq-CCOo_8",
        "outputId": "2d01fc42-1bdb-4b97-e1bf-af453ad6510d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting arch\n",
            "  Downloading arch-6.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (982 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from arch) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: arch\n",
            "Successfully installed arch-6.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas tensorflow arch keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "svakQUO6ZDZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2bddb7-b205-4cf3-fd0f-f05cd225d579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily"
      ],
      "metadata": {
        "id": "t6nRTqcNZz9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Load your data into a pandas DataFrame\n",
        "eth_data = pd.read_csv('/content/drive/My Drive/DS Masters/Data/Ethereum Historical Data.csv', index_col='Date', parse_dates=True)"
      ],
      "metadata": {
        "id": "oWXkBQqeP3RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eth_data.Price = eth_data.Price.str.replace(',', '').astype(float)"
      ],
      "metadata": {
        "id": "i05KKJl_T5oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arch import arch_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Calculate returns from the price data\n",
        "eth_data['returns'] = eth_data['Close'].pct_change().dropna()\n",
        "\n",
        "# Data Preparation for AP-GARCH and DFFNN\n",
        "returns = eth_data['returns'].values[1:]  # remove the first NA value\n",
        "\n",
        "\n",
        "# Step 1: Fit AP-GARCH Model\n",
        "def fit_apgarch(returns):\n",
        "    apgarch_model = arch_model(returns, vol='Garch', p=1, q=1, o=1, power=1.0)\n",
        "    res = apgarch_model.fit(update_freq=5, disp='off')\n",
        "    forecast = res.conditional_volatility\n",
        "    return forecast\n",
        "\n",
        "# Step 2: Prepare data for DFFNN\n",
        "def prepare_data_for_dffnn(returns, forecasted_volatility):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_returns = scaler.fit_transform(returns.reshape(-1, 1))\n",
        "    scaled_volatility = scaler.transform(forecasted_volatility.reshape(-1, 1))\n",
        "\n",
        "    # Create a feature set with returns and forecasted volatility\n",
        "    features = np.hstack((scaled_returns[:-1], scaled_volatility[:-1]))  # shift by one to predict next step\n",
        "    labels = scaled_returns[1:]\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Step 3: Define DFFNN\n",
        "def build_dffnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Main pipeline\n",
        "forecasted_volatility = fit_apgarch(returns)\n",
        "features, labels = prepare_data_for_dffnn(returns, forecasted_volatility)\n",
        "model = build_dffnn(features.shape[1])\n",
        "\n",
        "# Split data into training and testing sets (80-20 split for example)\n",
        "split = int(len(features) * 0.8)\n",
        "X_train, X_test = features[:split], features[split:]\n",
        "y_train, y_test = labels[:split], labels[split:]\n",
        "\n",
        "# Step 4: Train DFFNN\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluation\n",
        "mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MSE: {mse}\")\n",
        "\n",
        "# For a complete AP-DFFN model, you might iterate through the process, tweaking model parameters based on performance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK1BO9cdOy41",
        "outputId": "449fee5d-5a26-43bd-c51a-e8c69f640317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.0006593. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 100 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 2s 46ms/step - loss: 0.3379 - val_loss: 0.2176\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1513 - val_loss: 0.0971\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.0389\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.0199\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0192\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0126 - val_loss: 0.0188\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0114 - val_loss: 0.0179\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0108 - val_loss: 0.0178\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0176\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0106 - val_loss: 0.0174\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0173\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 0.0173\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0103 - val_loss: 0.0171\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0102 - val_loss: 0.0171\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0102 - val_loss: 0.0171\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0101 - val_loss: 0.0170\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0169\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 0.0169\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0100 - val_loss: 0.0169\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0169\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0101 - val_loss: 0.0169\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0169\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0172\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0173\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Test MSE: 0.021332966163754463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arch import arch_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming eth_data is your DataFrame with hourly Ethereum price data\n",
        "eth_data['returns'] = eth_data['Close'].pct_change().dropna()\n",
        "returns = eth_data['returns'].dropna().values\n",
        "\n",
        "# Helper function to split data\n",
        "def train_test_split(data, test_size=0.2):\n",
        "    split_idx = int(len(data) * (1 - test_size))\n",
        "    return data[:split_idx], data[split_idx:]\n",
        "\n",
        "# 1. GARCH Model\n",
        "def evaluate_garch(returns):\n",
        "    model = arch_model(returns, vol='Garch', p=1, q=1)\n",
        "    res = model.fit(disp='off')\n",
        "    forecast = res.conditional_volatility\n",
        "    y_true, y_pred = train_test_split(returns**2)  # Squaring returns as GARCH models the variance\n",
        "    mse = mean_squared_error(y_true[len(y_true) - len(y_pred):], y_pred**2)  # Squaring predictions to compare with true variance\n",
        "    print(f'GARCH Model MSE: {mse}')\n",
        "    return mse\n",
        "\n",
        "# 2. AP-GARCH Model\n",
        "def evaluate_apgarch(returns):\n",
        "    model = arch_model(returns, p=1, q=1, o=1, vol='Garch', power=1.0)\n",
        "    res = model.fit(disp='off')\n",
        "    forecast = res.conditional_volatility\n",
        "    y_true, y_pred = train_test_split(returns**2)\n",
        "    mse = mean_squared_error(y_true[len(y_true) - len(y_pred):], y_pred**2)\n",
        "    print(f'AP-GARCH Model MSE: {mse}')\n",
        "    return mse\n",
        "\n",
        "# 3. DFFNNs\n",
        "def evaluate_dffnns(returns):\n",
        "    scaler = MinMaxScaler()\n",
        "    returns_scaled = scaler.fit_transform(returns.reshape(-1, 1)).flatten()\n",
        "    X, y = returns_scaled[:-1], returns_scaled[1:]\n",
        "    X_train, X_test = train_test_split(X)\n",
        "    y_train, y_test = train_test_split(y)\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(1,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    predictions = model.predict(X_test).flatten()\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f'DFFNNs MSE: {mse}')\n",
        "    return mse\n",
        "\n",
        "# 4. AP-DFFN\n",
        "def evaluate_ap_dffn(returns):\n",
        "    # Assuming AP-GARCH volatility forecasts and DFFNNs are combined here\n",
        "    # This is a placeholder for your implementation based on the previous AP-DFFN guidelines\n",
        "    # Step 1: Fit AP-GARCH Model\n",
        "     def fit_apgarch(returns):\n",
        "        apgarch_model = arch_model(returns, vol='Garch', p=1, q=1, o=1, power=1.0)\n",
        "        res = apgarch_model.fit(update_freq=5, disp='off')\n",
        "        forecast = res.conditional_volatility\n",
        "        return forecast\n",
        "\n",
        "     # Step 2: Prepare data for DFFNN\n",
        "     def prepare_data_for_dffnn(returns, forecasted_volatility):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_returns = scaler.fit_transform(returns.reshape(-1, 1))\n",
        "        scaled_volatility = scaler.transform(forecasted_volatility.reshape(-1, 1))\n",
        "\n",
        "        # Create a feature set with returns and forecasted volatility\n",
        "        features = np.hstack((scaled_returns[:-1], scaled_volatility[:-1]))  # shift by one to predict next step\n",
        "        labels = scaled_returns[1:]\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "     # Step 3: Define DFFNN\n",
        "     def build_dffnn(input_shape):\n",
        "        model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "     # Main pipeline\n",
        "     forecasted_volatility = fit_apgarch(returns)\n",
        "     features, labels = prepare_data_for_dffnn(returns, forecasted_volatility)\n",
        "     model = build_dffnn(features.shape[1])\n",
        "\n",
        "     # Split data into training and testing sets (80-20 split for example)\n",
        "     split = int(len(features) * 0.8)\n",
        "     X_train, X_test = features[:split], features[split:]\n",
        "     y_train, y_test = labels[:split], labels[split:]\n",
        "\n",
        "     # Step 4: Train DFFNN\n",
        "     model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "\n",
        "     # Evaluation\n",
        "     mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "     print(f\"Test MSE: {mse}\")\n",
        "     return mse\n",
        "\n",
        "# Load your data\n",
        "#eth_data = pd.read_csv('your_ethereum_hourly_price_data.csv')  # Make sure to adjust the path\n",
        "\n",
        "# Evaluate Models\n",
        "garch_mse = evaluate_garch(returns)\n",
        "apgarch_mse = evaluate_apgarch(returns)\n",
        "dffnns_mse = evaluate_dffnns(returns)\n",
        "apdfnn_mse = evaluate_ap_dffn(returns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHmvWlYoZw1M",
        "outputId": "0f719fa3-f611-4ab3-bf1e-b62c60883422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.0006593. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 100 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.0006593. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 100 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GARCH Model MSE: 3.050549452698324e-06\n",
            "AP-GARCH Model MSE: 3.050549452698324e-06\n",
            "3/3 [==============================] - 0s 4ms/step\n",
            "DFFNNs MSE: 0.02092170474338698\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.0006593. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 100 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 43ms/step - loss: 0.2028 - val_loss: 0.1217\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0740 - val_loss: 0.0366\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0172 - val_loss: 0.0187\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0240\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0156 - val_loss: 0.0195\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 0.0182\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 0.0187\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0181\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0178\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0109 - val_loss: 0.0177\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0176\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0175\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0106 - val_loss: 0.0174\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0104 - val_loss: 0.0171\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0170\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0102 - val_loss: 0.0170\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0101 - val_loss: 0.0169\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 0.0168\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0168\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0169\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0172\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0100 - val_loss: 0.0169\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0169\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0169\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0170\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0171\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0171\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0170\n",
            "Test MSE: 0.02060544490814209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Log returns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from arch import arch_model\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "kB0CjKjhccX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "|def calculate_log_returns(prices):\n",
        "    \"\"\"\n",
        "    Calculate the daily log returns of a given series of prices.\n",
        "\n",
        "    Parameters:\n",
        "    prices (pd.Series): A pandas Series containing the daily closing prices of an asset.\n",
        "\n",
        "    Returns:\n",
        "    pd.Series: A pandas Series containing the daily log returns.\n",
        "    \"\"\"\n",
        "    # Calculate daily log returns\n",
        "    log_returns = np.log(prices / prices.shift(1))\n",
        "    return log_returns.dropna()  # Remove any NaN values that result from the shift\n",
        "\n",
        " # Calculate log returns\n",
        "log_returns_eth = calculate_log_returns(eth_data['Price'].sort_index())\n",
        "print(log_returns_eth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y5Ai7JoOpnI",
        "outputId": "05119f0d-6e31-4ea1-f399-5550e3dc5651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date\n",
            "2016-03-11    0.016878\n",
            "2016-03-12    0.078045\n",
            "2016-03-13    0.153930\n",
            "2016-03-14   -0.186977\n",
            "2016-03-15    0.043825\n",
            "                ...   \n",
            "2024-04-10    0.010834\n",
            "2024-04-11   -0.012092\n",
            "2024-04-12   -0.077961\n",
            "2024-04-13   -0.075439\n",
            "2024-04-14    0.016328\n",
            "Name: Price, Length: 2957, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_returns_eth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjtPsIPoPobZ",
        "outputId": "f0343cd8-8f01-4932-87e7-65c4671938bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2016-03-11    0.016878\n",
              "2016-03-12    0.078045\n",
              "2016-03-13    0.153930\n",
              "2016-03-14   -0.186977\n",
              "2016-03-15    0.043825\n",
              "                ...   \n",
              "2024-04-10    0.010834\n",
              "2024-04-11   -0.012092\n",
              "2024-04-12   -0.077961\n",
              "2024-04-13   -0.075439\n",
              "2024-04-14    0.016328\n",
              "Name: Price, Length: 2957, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from arch import arch_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calculate_log_returns(prices):\n",
        "    \"\"\"Calculate daily log returns from prices.\"\"\"\n",
        "    return np.log(prices / prices.shift(1)).dropna()\n",
        "\n",
        "def fit_garch_model(log_returns, p_orders, q_orders):\n",
        "    \"\"\"Fit GARCH models and perform parameter search for best MSE.\"\"\"\n",
        "    best_mse = np.inf\n",
        "    best_model = None\n",
        "    best_order = None\n",
        "    #log_returns = 10*log_returns\n",
        "    for p in p_orders:\n",
        "        for q in q_orders:\n",
        "            model = arch_model(log_returns, vol='Garch', p=p, q=q, dist='Normal')\n",
        "            res = model.fit(disp='off')\n",
        "            # Calculate the predicted conditional volatility\n",
        "            predicted_volatility = res.conditional_volatility\n",
        "            # Calculate MSE comparing squared log returns to squared predicted volatilities\n",
        "            mse = mean_squared_error(log_returns**2, predicted_volatility**2)\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_model = res\n",
        "                best_order = (p, q)\n",
        "\n",
        "    return best_model, best_order, best_mse\n",
        "\n",
        "\n",
        "log_returns = calculate_log_returns(eth_data['Price'].sort_index())\n",
        "best_model, best_order, best_mse = fit_garch_model(log_returns, p_orders=[1, 2], q_orders=[1, 2])\n",
        "\n",
        "print(\"Best Model Order (p, q):\", best_order)\n",
        "print(\"Best Model Summary:\")\n",
        "print(best_model.summary())\n",
        "print(\"Best mse:\", best_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_RWWNLTUMv3",
        "outputId": "6a279dd8-f5f1-46b7-ca89-11831c5bccb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Order (p, q): (2, 2)\n",
            "Best Model Summary:\n",
            "                     Constant Mean - GARCH Model Results                      \n",
            "==============================================================================\n",
            "Dep. Variable:                  Price   R-squared:                       0.000\n",
            "Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n",
            "Vol Model:                      GARCH   Log-Likelihood:                4830.15\n",
            "Distribution:                  Normal   AIC:                          -9648.30\n",
            "Method:            Maximum Likelihood   BIC:                          -9612.35\n",
            "                                        No. Observations:                 2957\n",
            "Date:                Sun, Apr 14 2024   Df Residuals:                     2956\n",
            "Time:                        16:55:33   Df Model:                            1\n",
            "                                 Mean Model                                 \n",
            "============================================================================\n",
            "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
            "----------------------------------------------------------------------------\n",
            "mu         1.5567e-03  7.731e-04      2.014  4.406e-02 [4.140e-05,3.072e-03]\n",
            "                               Volatility Model                              \n",
            "=============================================================================\n",
            "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
            "-----------------------------------------------------------------------------\n",
            "omega      7.4662e-05  5.188e-05      1.439      0.150 [-2.702e-05,1.763e-04]\n",
            "alpha[1]       0.1273  4.261e-02      2.987  2.816e-03    [4.377e-02,  0.211]\n",
            "alpha[2]   6.9178e-12      0.111  6.247e-11      1.000      [ -0.217,  0.217]\n",
            "beta[1]        0.6489      0.382      1.697  8.963e-02      [ -0.100,  1.398]\n",
            "beta[2]        0.2051      0.302      0.678      0.498      [ -0.388,  0.798]\n",
            "=============================================================================\n",
            "\n",
            "Covariance estimator: robust\n",
            "Best mse: 8.09836988139969e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.002728. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 10 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.002728. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 10 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.002728. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 10 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
            "estimating the model parameters. The scale of y is 0.002728. Parameter\n",
            "estimation work better when this value is between 1 and 1000. The recommended\n",
            "rescaling is 10 * y.\n",
            "\n",
            "This warning can be disabled by either rescaling y before initializing the\n",
            "model or by setting rescale=False.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install statsmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM-k4sTKWQoq",
        "outputId": "44744ee4-20ab-4cde-9c84-e924679fa887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HAR\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calculate_log_returns(prices):\n",
        "    \"\"\"Calculate daily log returns from prices.\"\"\"\n",
        "    return np.log(prices / prices.shift(1)).dropna()\n",
        "\n",
        "def prepare_har_data(log_returns, daily_lag=1, weekly_lag=5, monthly_lag=22):\n",
        "    \"\"\"Prepare data for HAR model with specified lags.\"\"\"\n",
        "    df = pd.DataFrame(log_returns.values, columns=['log_return'])\n",
        "    df['daily'] = df['log_return'].shift(daily_lag)\n",
        "    df['weekly'] = df['log_return'].rolling(window=weekly_lag).mean().shift(1)\n",
        "    df['monthly'] = df['log_return'].rolling(window=monthly_lag).mean().shift(1)\n",
        "    df.dropna(inplace=True)  # Drop rows with NaN values\n",
        "    return df\n",
        "\n",
        "def har_model(df):\n",
        "    \"\"\"Fit HAR model and return model and predictions.\"\"\"\n",
        "    X = df[['daily', 'weekly', 'monthly']]\n",
        "    y = df['log_return'].dropna()\n",
        "    X = sm.add_constant(X)  # adding a constant\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    predictions = model.predict(X)\n",
        "    return model, predictions\n",
        "\n",
        "def parameter_search(prices, daily_range, weekly_range, monthly_range):\n",
        "    \"\"\"Search for best parameters based on MSE.\"\"\"\n",
        "    log_returns = calculate_log_returns(prices)\n",
        "    #\n",
        "    best_mse = float('inf')\n",
        "    best_params = {}\n",
        "\n",
        "    for d in daily_range:\n",
        "        for w in weekly_range:\n",
        "            for m in monthly_range:\n",
        "                df = prepare_har_data(log_returns, daily_lag=d, weekly_lag=w, monthly_lag=m)\n",
        "                _, predictions = har_model(df)\n",
        "                mse = mean_squared_error(df['log_return'], predictions)\n",
        "                if mse < best_mse:\n",
        "                    best_mse = mse\n",
        "                    best_params = {'daily': d, 'weekly': w, 'monthly': m, 'MSE': mse}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "best_params = parameter_search(eth_data['Price'].sort_index(), daily_range=[1], weekly_range=[5], monthly_range=[22])\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwElZXD6VH_8",
        "outputId": "999475ba-c0a6-4396-c9a2-25e03158c1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'daily': 1, 'weekly': 5, 'monthly': 22, 'MSE': 0.002674285431137921}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def calculate_log_returns(prices):\n",
        "    \"\"\"Calculate daily log returns from prices.\"\"\"\n",
        "    return np.log(prices / prices.shift(1)).dropna()\n",
        "\n",
        "def build_model(n_layers, n_units, input_shape, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"Builds a Sequential neural network model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_units, activation='relu', input_shape=(input_shape,)))\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(n_units, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def parameter_search(data, n_units_options, n_layers_options):\n",
        "    \"\"\"Searches for the best neural network architecture.\"\"\"\n",
        "    X = data.drop('log_return', axis=1)\n",
        "    y = data['log_return']**2  # Predicting the squared returns as a proxy for volatility\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for n_units in n_units_options:\n",
        "        for n_layers in n_layers_options:\n",
        "            model = build_model(n_layers, n_units, input_shape=X_train.shape[1])\n",
        "            model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "            predictions = model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test.fillna(0), predictions)\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_params = {'n_layers': n_layers, 'n_units': n_units, 'MSE': mse}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "# Example usage:\n",
        "df['log_return'] = calculate_log_returns(eth_data['Price'].sort_index())\n",
        "\n",
        "# Assuming we want to predict volatility from the squared returns\n",
        "best_params = parameter_search(df, n_units_options=[10, 50], n_layers_options=[1, 2])\n",
        "print(\"Best model parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr2TLEfhWsM7",
        "outputId": "fe6382ae-cc35-4b0d-e1fe-bc9194540978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "Best model parameters: {'n_layers': 1, 'n_units': 50, 'MSE': 0.017908950058498414}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def calculate_log_returns(prices):\n",
        "    \"\"\"Calculate daily log returns from prices.\"\"\"\n",
        "    return np.log(prices / prices.shift(1)).dropna()\n",
        "\n",
        "def build_dfnn(n_layers, n_units, input_shape, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"Builds a Deep Feedforward Neural Network (DFNN).\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n_units, activation='relu', input_shape=(input_shape,)))\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(n_units, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def parameter_search(data, n_units_options, n_layers_options):\n",
        "    \"\"\"Searches for the best neural network architecture based on MSE.\"\"\"\n",
        "    X = data.drop('log_return', axis=1)\n",
        "    y = data['log_return']**2  # Predicting squared returns as a proxy for volatility\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for n_units in n_units_options:\n",
        "        for n_layers in n_layers_options:\n",
        "            model = build_dfnn(n_layers, n_units, input_shape=X_train.shape[1])\n",
        "            model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "            predictions = model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test.fillna(0), predictions)\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_params = {'n_layers': n_layers, 'n_units': n_units, 'MSE': mse}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "df['log_return'] = calculate_log_returns(eth_data['Price'].sort_index())\n",
        "\n",
        "# Parameter search for optimal DFNN architecture\n",
        "best_params = parameter_search(df, n_units_options=[10, 50], n_layers_options=[1, 3])\n",
        "print(\"Best model parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7-W-OGehVCf",
        "outputId": "8de57cd7-def1-4d28-fbe1-889f7d7b6d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "Best model parameters: {'n_layers': 1, 'n_units': 50, 'MSE': 0.0006727218500663964}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def calculate_log_returns(prices):\n",
        "    \"\"\"Calculate daily log returns from prices.\"\"\"\n",
        "    return np.log(prices / prices.shift(1)).dropna()\n",
        "\n",
        "def build_lstm(n_units, input_shape, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"Builds an LSTM network.\"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(n_units, input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(dropout_rate),\n",
        "        LSTM(n_units),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def parameter_search(data, n_units_options):\n",
        "    \"\"\"Searches for the best LSTM architecture based on MSE.\"\"\"\n",
        "    X = np.array(data.drop('log_return', axis=1))\n",
        "    y = np.array(data['log_return']**2)  # Predicting squared returns as a proxy for volatility\n",
        "    # Reshape X for LSTM [samples, time steps, features]\n",
        "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for n_units in n_units_options:\n",
        "        model = build_lstm(n_units, input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "            best_params = {'n_units': n_units, 'MSE': mse}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "# Example usage:\n",
        "df['log_return'] = calculate_log_returns(eth_data['Price'].sort_index())\n",
        "\n",
        "# Assuming we want to predict volatility from squared returns\n",
        "best_params = parameter_search(df, n_units_options=[10, 20, 30, 40, 50, 100, 200, 500, 1000])\n",
        "print(\"Best model parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdFGux_MizkX",
        "outputId": "216960a6-0b80-4400-b257-8a2147309f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 781ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 779ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 781ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 789ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Best model parameters: {'n_units': 10, 'MSE': 5.443004211736995e-06}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CaZmWph2jf8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}