{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVoCFzHvbfbpMxFk0lhx5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atacankavdir/academic/blob/main/volatility_baseline_modelrun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY6dFBnrPIFw",
        "outputId": "76a13dc7-834a-4ccf-dd67-c80b475f0719"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXJq-CCOo_8",
        "outputId": "c912f9db-fea8-42be-e06c-fb48c03c8e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting arch\n",
            "  Downloading arch-6.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (982 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas tensorflow arch keras sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "svakQUO6ZDZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Load your data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/drive/My Drive/DS Masters/Data/ETH-USD.csv', index_col='Date', parse_dates=True)"
      ],
      "metadata": {
        "id": "oWXkBQqeP3RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arch import arch_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Calculate returns from the price data\n",
        "eth_data['returns'] = eth_data['price'].pct_change().dropna()\n",
        "\n",
        "# Data Preparation for AP-GARCH and DFFNN\n",
        "returns = eth_data['returns'].values[1:]  # remove the first NA value\n",
        "\n",
        "# Step 1: Fit AP-GARCH Model\n",
        "def fit_apgarch(returns):\n",
        "    apgarch_model = arch_model(returns, vol='Garch', p=1, q=1, o=1, power=1.0)\n",
        "    res = apgarch_model.fit(update_freq=5, disp='off')\n",
        "    forecast = res.conditional_volatility\n",
        "    return forecast\n",
        "\n",
        "# Step 2: Prepare data for DFFNN\n",
        "def prepare_data_for_dffnn(returns, forecasted_volatility):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_returns = scaler.fit_transform(returns.reshape(-1, 1))\n",
        "    scaled_volatility = scaler.transform(forecasted_volatility.reshape(-1, 1))\n",
        "\n",
        "    # Create a feature set with returns and forecasted volatility\n",
        "    features = np.hstack((scaled_returns[:-1], scaled_volatility[:-1]))  # shift by one to predict next step\n",
        "    labels = scaled_returns[1:]\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Step 3: Define DFFNN\n",
        "def build_dffnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Main pipeline\n",
        "forecasted_volatility = fit_apgarch(returns)\n",
        "features, labels = prepare_data_for_dffnn(returns, forecasted_volatility)\n",
        "model = build_dffnn(features.shape[1])\n",
        "\n",
        "# Split data into training and testing sets (80-20 split for example)\n",
        "split = int(len(features) * 0.8)\n",
        "X_train, X_test = features[:split], features[split:]\n",
        "y_train, y_test = labels[:split], labels[split:]\n",
        "\n",
        "# Step 4: Train DFFNN\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluation\n",
        "mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MSE: {mse}\")\n",
        "\n",
        "# For a complete AP-DFFN model, you might iterate through the process, tweaking model parameters based on performance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "RK1BO9cdOy41",
        "outputId": "408b22d1-8dba-45c4-9237-4a4fe8e2c606"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eth_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-85a3c76ec1ed>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calculate returns from the price data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0meth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpct_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Data Preparation for AP-GARCH and DFFNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eth_data' is not defined"
          ]
        }
      ]
    }
  ]
}